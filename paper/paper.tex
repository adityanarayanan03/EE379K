\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}

\begin{document}

\title{Human Pose Estimation for Video Game Control}
\author{Rakesh Johny, Tom Li, Aditya Narayanan, Albert Xia}
\maketitle

\begin{abstract}
    Current methods of interacting with computers are flawed in a 
    couple of key ways: they fail to map physically-intuitive motions to their computer 
    control counterparts, and they rely heavily on a user's fine-motor skills, 
    which are heavily impacted by factors such as muscle coordination disabilities 
    and old age. In this paper, we propose a system of computer control through 
    gesture tracking via real-time human pose estimation on an embedded device, 
    which is capable of addresssing these issues, by mapping general physically-intuitive 
    motions into computer control. Furthermore, our device boasts low latency in 
    the human-computer interaction, and is minimally intrustive to the computer being 
    controlled. We test the viability of our system as a computer-control device by playing two 
    video games using only gestures.
\end{abstract}

\section{Introduction}
The human-computer interfaces behind many modern video games use either handheld 
joystick controllers or keyboard and mouse input, which are examples
of typical human input device for computers, monitors, televisions, etc. There are two 
major drawbacks to traditional human input systems that are often overlooked: the input needed 
to achieve a certain effect has little to no correspondence to a physically-intuitive set 
of motions, and more importantly, inputs are heavily dependent on the user's fine motor skills. 
Fine motor skills used to interact with modern technology are heavily affected by factors such 
as muscular coordination disorders and old age. Computer vision, particularly the processes 
of human pose estimation and motion tracking may allow us to create human-computer interfaces 
that link computer control to physically-intuitive human motion inputs with little dependence on 
fine motor ability or additional physical input devices. We propose a vision-based computer-input 
system that maps gestures and motion of the user's body into computer input. We test the system's 
effectiveness with the playability of two simple video games as metrics. In order to demonstrate 
the versatility of our phased-approach to gesture-based control, we select a car-racing game and 
the Google Chrome Dino Racer game as test video games. Such a system, when expanded to replicating 
general keyboard + mouse control, would be instrumental in improving the accessibility of 
modern technology, with implementations far beyond the scope of computer games.

\section{Related Work}
\subsection{Human Pose Estimation}
Some of the largest components of our system fall into the category of so called Human 
Pose Estimation or the ability to properly:

\begin{itemize}
    \item Identify users as sources of input, from a video stream
    \item Segment the user's body to isolate relevant portions of data
    \item Compute the location, in image coordinates, of keypoints on the user's body.
\end{itemize}

The area of Human Pose Estimation is widely studied. There are a wide array of robust, optimized 
solutions for gathering 2D image coordinates of keypoints from images of users. 

\subsubsection{Openpose}
One popular solution to realtime human pose estimation, derived from \cite{8765346}, 
uses Part Affinity Fields (PAFs) to learn to assosciate body parts with individuals in the image, 
and open-source code for the so called openpose library shows promising results for real-time 
human body segmentation. 

\begin{figure}[h]
    \centering
    \includegraphics[width = .8\linewidth]{images/openpose.png}
    \caption{A demonstration of openpose performing human body segmentation.}
\end{figure}

\subsubsection{MoveNet and PoseNet}

PoseNet and its more recent counterpart MoveNet are fully convolutional neural network pose 
estimators built on existing ResNet or MobileNet backbones (citation here). Benchmarks of PoseNet and MoveNet show that it is 
viable to run either estimator in real time on embedded devices with appropriate 
model quantizations (citation here), with PoseNet being slightly faster when compared to MoveNet at identical 
levels of quantization. In comparison's with Openpose, PoseNet/MoveNet are shown to perform 
slightly better on instance segmentation of the person class from the COCO dataset.

\subsection{Gesture Identification/Motion Tracking}
Openpose and MoveNet/PoseNet give us promising ways to identify locations of human body features 
(hands, arms, etc.) in a video stream. However, this is not enough to categorize a user's motion 
as a particular command to a computer system. To be able to extract feature motion over time 
as a useful input to our car racing video game, we will have to implement motion tracking 
over a video stream.\\ 

A number of guides exist for implementing feature tracking over video. \cite{tracking_1} 
and \cite{tracking_2} demonstrate simple object tracking across video frames, and \cite{tracking_2} 
demonstrates a system shown to be accurate with motion of hands. We drew on \cite{tracking_1} and 
\cite{tracking_2} in developing a tracking algorithm for the identification of a human user ducking 
or jumping from a keypoint cloud.

\section{Methodology}
Our human computer interface consists of three major steps: Pose Estimation, or feature 
location, Feature Tracking and classification of motions, and video game interfacing. In addition, 
we adopt a multi-phased approach to our human computer interface. Phase 1 consists of a robust and 
general human pose estimator and keypoint detector. The work in Phase 1 is task/game agnostic, as 
we implement identical versions of this phase for the car-racer game as well as the chrome 
dinosaur game. Phase 2 is a game-specific keypoint classifier, where the output from Phase 1 is 
classified into a number of computer commands in a way dependent on the physical motions being 
replicated in the game as well as the computer commands the game is traditionally played with.\\

The motivation behind such a two phase approach rather than building a classifier that directly 
maps input video streams to computer commands is to improve the versatility of our human-computer 
interaction system. A fully general Phase 1 implemented independent of the game in question allows 
the system to be more easily adapted to any particular set of computer commands. We demonstrate the 
versatility of this system through the implementation of our human-computer interface for two 
distinct video games that utilize identical Phase 1 code.

\subsection{Testing Pose Estimation Methods}
Before we develop phase 1 of our human-computer interface, we test a number of proposed solutions 
for human pose estimation to determine which ones are best-suited to our needs. The main criteria 
we require for our human pose estimator are:

\begin{itemize}
    \item Real time operation. The need for this criterion is obvious, as we require low latency 
        classification in order to mimic low-latency commmands for video games. Essentially, the user 
        should feel as if the delay between executing a physical command and the video game reflecting 
        such a command is instantaneous.
    \item The system should be minimally intrusive on the computer used to play the game. This 
        essentially means that we cannot use computing resources so significant that usability of 
        the computer itself is diminished. 
\end{itemize}

\subsubsection{Openpose Python API}
The openpose package offers a Python API for implementing inference over video feeds using one 
of the openpose models. We began testing the openpose Python API for our task, but found that 
installing the API resulted in dependency issues accross multiple team members' computers and 
decided to instead test the openpose pretrained models implemented independently of the openpose 
python API.

%https://learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/
\subsubsection{Openpose model implemented in OpenCV}
(citation) offers a method to perform inference on CPU/GPU using pretrained versions of the 
openpose hand/body pose estimators. For inference with the openpose pretrained model through 
OpenCV, we observed only 1-2 FPS on a 2.3 GHz Intel i9 CPU. We expect that running the same model 
with CUDA acceleration would result in true real-time performance, but as one of our design 
constraints for our system, we imposed that we cannot use significant GPU resources in order to 
maintain minimal intrusiveness onto the gaming machine.

\subsubsection{PoseNet and the Jetson Nano}
Due to the poor performance of the Openpose models, we began to look at different methods for 
real-time pose estimation. One option that was promising was to completely offload the task of 
human pose estimation to an edge-device to reduce the computational burden on the device used 
to play the games. Such an embedded device would ideally be compact yet powerful enough to run 
quantized versions of pose estimators in real time. We found the NVIDIA Jetson Nano to be an 
ideal candidate for such an embedded device.\\

With a 128-core NVIDIA GPU on a single-board computer footprint, the Jetson Nano is a popular 
choice for deep model inference on embedded computers. Perhaps more important than the hardware 
itself, however, is the specific Tensor RT optimized models available for use on the Jetson Nano. 
Through a number of optimization techniques such as model quantization, unuzed output elimination 
and horizontal layer fusion/layer aggregation, the Tensorflow with Tensor RT (TF-TRT) backend is 
able to greatly improve inference time on the Jetson Nano. TF-TRT is able to output optimized 
models from Open Neural Network Exchange Format (ONXX) and for popular tasks such as human pose 
estimation, ONXX model formats are readily available from NVIDIA Developers. 

\subsection{Pose Estimation}
Using a variant of PoseNet built on a ResNet-18 backbone, we observe 15-17 Frames Per Second 
(FPS) on the Jetson Nano. The so called Pose-ResNet-18-body outputs 17 detected keypoints in 
image coordinate format. 15-17 FPS performance is smooth enough keypoint detection to allow us to 
play these simple games in real time.

\subsection{Feature Tracking and Classification}
\subsubsection{Feature Classification For Steering}
\subsubsection{Feature Classification For Jumping/Ducking} 

\subsection{Game Interface}

\section{Results}

\section{Conclusion and Future Works}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{proposal_bib}
}

\end{document}